{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Autopilot Candidate Definition Notebook\n",
    "\n",
    "This notebook was automatically generated by the AutoML job **automl-woobin**.\n",
    "This notebook allows you to customize the candidate definitions and execute the SageMaker Autopilot workflow.\n",
    "\n",
    "The dataset has **21** columns and the column named **Churn?** is used as\n",
    "the target column. This is being treated as a **BinaryClassification** problem. The dataset also has **2** classes.\n",
    "This notebook will build a **[BinaryClassification](https://en.wikipedia.org/wiki/Binary_classification)** model that\n",
    "**maximizes** the \"**F1**\" quality metric of the trained models.\n",
    "The \"**F1**\" metric applies for binary classification with a positive and negative class. It mixes between precision and recall, and is recommended in cases where there are more negative examples compared to positive examples.\n",
    "\n",
    "As part of the AutoML job, the input dataset has been randomly split into two pieces, one for **training** and one for\n",
    "**validation**. This notebook helps you inspect and modify the data transformation approaches proposed by Amazon SageMaker Autopilot. You can interactively\n",
    "train the data transformation models and use them to transform the data. Finally, you can execute a multiple algorithm hyperparameter optimization (multi-algo HPO)\n",
    "job that helps you find the best model for your dataset by jointly optimizing the data transformations and machine learning algorithms.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Available Knobs</strong>\n",
    "Look for sections like this for recommended settings that you can change.\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Sagemaker Setup](#Sagemaker-Setup)\n",
    "    1. [Downloading Generated Candidates](#Downloading-Generated-Modules)\n",
    "    1. [SageMaker Autopilot Job and Amazon Simple Storage Service (Amazon S3) Configuration](#SageMaker-Autopilot-Job-and-Amazon-Simple-Storage-Service-(Amazon-S3)-Configuration)\n",
    "1. [Candidate Pipelines](#Candidate-Pipelines)\n",
    "    1. [Generated Candidates](#Generated-Candidates)\n",
    "    1. [Selected Candidates](#Selected-Candidates)\n",
    "1. [Executing the Candidate Pipelines](#Executing-the-Candidate-Pipelines)\n",
    "    1. [Run Data Transformation Steps](#Run-Data-Transformation-Steps)\n",
    "    1. [Multi Algorithm Hyperparameter Tuning](#Multi-Algorithm-Hyperparameter-Tuning)\n",
    "1. [Model Selection and Deployment](#Model-Selection-and-Deployment)\n",
    "    1. [Tuning Job Result Overview](#Tuning-Job-Result-Overview)\n",
    "    1. [Model Deployment](#Model-Deployment)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Setup\n",
    "\n",
    "Before you launch the SageMaker Autopilot jobs, we'll setup the environment for Amazon SageMaker\n",
    "- Check environment & dependencies.\n",
    "- Create a few helper objects/function to organize input/output data and SageMaker sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimal Environment Requirements**\n",
    "\n",
    "- Jupyter: Tested on `JupyterLab 1.0.6`, `jupyter_core 4.5.0` and `IPython 6.4.0`\n",
    "- Kernel: `conda_python3`\n",
    "- Dependencies required\n",
    "  - `sagemaker-python-sdk>=2.40.0`\n",
    "    - Use `!pip install sagemaker==2.40.0` to download this dependency.\n",
    "    - Kernel may need to be restarted after download.\n",
    "- Expected Execution Role/permission\n",
    "  - S3 access to the bucket that stores the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Generated Modules\n",
    "Download the generated data transformation modules and an SageMaker Autopilot helper module used by this notebook.\n",
    "Those artifacts will be downloaded to **automl-woobin-artifacts** folder.\n",
    "\n",
    "생성된 데이터 변환 모듈과 이 노트북에 사용되는 SageMaker 오토파일럿 도우미 모듈을 다운로드합니다.\n",
    "이러한 아티팩트는 *automl-woobin-artifacts* 폴더로 다운로드됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p automl-woobin-artifacts\n",
    "!aws s3 sync s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/sagemaker-automl-candidates/automl-woobin-pr-1-534756e62f4447ac91291d1363d87182be4ed7311617/generated_module automl-woobin-artifacts/generated_module --only-show-errors\n",
    "!aws s3 sync s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/sagemaker-automl-candidates/automl-woobin-pr-1-534756e62f4447ac91291d1363d87182be4ed7311617/notebooks/sagemaker_automl automl-woobin-artifacts/sagemaker_automl --only-show-errors\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"automl-woobin-artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Autopilot Job and Amazon Simple Storage Service (Amazon S3) Configuration\n",
    "\n",
    "The following configuration has been derived from the SageMaker Autopilot job. These items configure where this notebook will\n",
    "look for generated candidates, and where input and output data is stored on Amazon S3.\n",
    "\n",
    "다음 구성은 SageMaker 자동 조종 작업에서 파생되었습니다. 이 항목은 이 노트북의 위치를 구성합니다\n",
    "생성된 후보와 입력 및 출력 데이터가 Amazon S3에 저장된 위치를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This notebook is initialized to use the following configuration: \n",
       "        <table>\n",
       "        <tr><th colspan=2>Name</th><th>Value</th></tr>\n",
       "        <tr><th>General</th><th>Role</th><td>arn:aws:iam::938531851447:role/service-role/AmazonSageMaker-ExecutionRole-20230202T005614</td></tr>\n",
       "        <tr><th rowspan=2>Base AutoML Job</th><th>Job Name</th><td>automl-woobin</td></tr>\n",
       "        <tr><th>Base Output S3 Path</th><td>s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin</td></tr>\n",
       "        <tr><th rowspan=5>Interactive Job</th><th>Job Name</th><td>automl-woo-notebook-run-15-14-44-38</td></tr>\n",
       "        <tr><th>Base Output S3 Path</th><td>s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/automl-woo-notebook-run-15-14-44-38</td></tr>\n",
       "        <tr><th>Data Processing Trained Model Directory</th><td>s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/automl-woo-notebook-run-15-14-44-38/data-processor-models</td></tr>\n",
       "        <tr><th>Data Processing Transformed Output</th><td>s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/automl-woo-notebook-run-15-14-44-38/transformed-data</td></tr>\n",
       "        <tr><th>Algo Tuning Model Output Directory</th><td>s3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/automl-woo-notebook-run-15-14-44-38/multi-algo-tuning</td></tr>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker_automl import uid, AutoMLLocalRunConfig\n",
    "\n",
    "# Where the preprocessed data from the existing AutoML job is stored\n",
    "BASE_AUTOML_JOB_NAME = 'automl-woobin'\n",
    "BASE_AUTOML_JOB_CONFIG = {\n",
    "    'automl_job_name': BASE_AUTOML_JOB_NAME,\n",
    "    'automl_output_s3_base_path': 's3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin',\n",
    "    'data_transformer_image_repo_version': '2.5-1-cpu-py3',\n",
    "    'algo_image_repo_versions': {'xgboost': '1.3-1-cpu-py3', 'linear-learner': 'training-cpu', 'mlp': 'training-cpu'},\n",
    "    'algo_inference_image_repo_versions': {'xgboost': '1.3-1-cpu-py3', 'linear-learner': 'inference-cpu', 'mlp': 'inference-cpu'}\n",
    "}\n",
    "\n",
    "# Path conventions of the output data storage path from the local AutoML job run of this notebook\n",
    "LOCAL_AUTOML_JOB_NAME = 'automl-woo-notebook-run-{}'.format(uid())\n",
    "LOCAL_AUTOML_JOB_CONFIG = {\n",
    "    'local_automl_job_name': LOCAL_AUTOML_JOB_NAME,\n",
    "    'local_automl_job_output_s3_base_path': 's3://sagemaker-ap-northeast-2-938531851447/automl-house-price/output/automl-woobin/{}'.format(LOCAL_AUTOML_JOB_NAME),\n",
    "    'data_processing_model_dir': 'data-processor-models',\n",
    "    'data_processing_transformed_output_dir': 'transformed-data',\n",
    "    'multi_algo_tuning_output_dir': 'multi-algo-tuning'\n",
    "}\n",
    "\n",
    "AUTOML_LOCAL_RUN_CONFIG = AutoMLLocalRunConfig(\n",
    "    role='arn:aws:iam::938531851447:role/service-role/AmazonSageMaker-ExecutionRole-20230202T005614',\n",
    "    base_automl_job_config=BASE_AUTOML_JOB_CONFIG,\n",
    "    local_automl_job_config=LOCAL_AUTOML_JOB_CONFIG,\n",
    "    security_config={'EnableInterContainerTrafficEncryption': False, 'VpcConfig': {}})\n",
    "\n",
    "AUTOML_LOCAL_RUN_CONFIG.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Pipelines\n",
    "\n",
    "The `AutoMLLocalRunner` keeps track of selected candidates and automates many of the steps needed to execute feature engineering and tuning steps.\n",
    "\n",
    "AutoMLlocalRunner는 선택된 후보를 추적하고 기능 엔지니어링 및 튜닝 단계를 실행하는 데 필요한 많은 단계를 자동화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n",
    "\n",
    "automl_interactive_runner = AutoMLInteractiveRunner(AUTOML_LOCAL_RUN_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Candidates\n",
    "\n",
    "The SageMaker Autopilot Job has analyzed the dataset and has generated **10** machine learning\n",
    "pipeline(s) that use **3** algorithm(s). Each pipeline contains a set of feature transformers and an\n",
    "algorithm.\n",
    "\n",
    "세이지메이커 오토파일럿 작업은 데이터 세트를 분석하고 3개의 알고리듬을 사용하는 10개의 기계 학습 파이프라인을 생성했다. 각 파이프라인에는 기능 변환기 세트와 알고리즘이 포함되어 있습니다.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Available Knobs</strong>\n",
    "\n",
    "1. The resource configuration: instance type & count\n",
    "1. Select candidate pipeline definitions by cells\n",
    "1. The linked data transformation script can be reviewed and updated. Please refer to the [README.md](./automl-woobin-artifacts/generated_module/README.md) for detailed customization instructions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp0-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp0.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:47:08,471 INFO root: Warning: pipeline candidate dpp0-xgboost has already been selected, replacing\n",
      "2023-02-15 14:47:08,520 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 14:47:08,536 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 14:47:08,568 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "automl_interactive_runner.select_candidate({\n",
    "    \"data_transformer\": {\n",
    "        \"name\": \"dpp0\",\n",
    "        \"training_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "            \"volume_size_in_gb\":  50\n",
    "        },\n",
    "        \"transform_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "        },\n",
    "        \"transforms_label\": True,\n",
    "        \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "        \"sparse_encoding\": True\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"name\": \"xgboost\",\n",
    "        \"training_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "        },\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp1-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp1.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustPCA](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/decomposition/robust_pca.py) followed by [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp1\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"text/csv\",\n",
    "#         \"sparse_encoding\": False\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp2-linear-learner](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp2.py)**: This data transformation strategy first transforms 'numeric' features using [combined RobustImputer and RobustMissingIndicator](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py) followed by [QuantileExtremeValuesTransformer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustPCA](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/decomposition/robust_pca.py) followed by [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *linear-learner* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:47:12,497 INFO root: Warning: pipeline candidate dpp2-linear-learner has already been selected, replacing\n",
      "2023-02-15 14:47:12,525 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 14:47:12,527 INFO sagemaker.image_uris: Same images used for training and inference. Defaulting to image scope: inference.\n",
      "2023-02-15 14:47:12,528 WARNING sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: training-cpu.\n",
      "2023-02-15 14:47:12,546 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 14:47:12,548 INFO sagemaker.image_uris: Same images used for training and inference. Defaulting to image scope: inference.\n",
      "2023-02-15 14:47:12,549 WARNING sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: inference-cpu.\n",
      "2023-02-15 14:47:12,568 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "automl_interactive_runner.select_candidate({\n",
    "    \"data_transformer\": {\n",
    "        \"name\": \"dpp2\",\n",
    "        \"training_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "            \"volume_size_in_gb\":  50\n",
    "        },\n",
    "        \"transform_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "        },\n",
    "        \"transforms_label\": True,\n",
    "        \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "        \"sparse_encoding\": False\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"name\": \"linear-learner\",\n",
    "        \"training_resource_config\": {\n",
    "            \"instance_type\": \"ml.m5.4xlarge\",\n",
    "            \"instance_count\": 1,\n",
    "        },\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp3-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp3.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp3\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp4-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp4.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp4\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp5-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp5.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp5\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp6-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp6.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp6\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp7-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp7.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp7\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp8-xgboost](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp8.py)**: This data transformation strategy first transforms 'numeric' features using [RobustImputer (converts missing values to nan)](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py), 'categorical' features using [ThresholdOneHotEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). It merges all the generated features and applies [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py). The\n",
    "transformed data will be used to tune a *xgboost* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp8\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"application/x-recordio-protobuf\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"xgboost\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[dpp9-mlp](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp9.py)**: This data transformation strategy transforms 'categorical' features using [ThresholdOrdinalEncoder](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/encoders.py), 'numeric' features using [RobustImputer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/impute/base.py) followed by [RobustStandardScaler](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/preprocessing/data.py), 'text' features using [MultiColumnTfidfVectorizer](https://github.com/aws/sagemaker-scikit-learn-extension/blob/master/src/sagemaker_sklearn_extension/feature_extraction/text.py). The\n",
    "transformed data will be used to tune a *mlp* model. Here is the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl_interactive_runner.select_candidate({\n",
    "#     \"data_transformer\": {\n",
    "#         \"name\": \"dpp9\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#             \"volume_size_in_gb\":  50\n",
    "#         },\n",
    "#         \"transform_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.4xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"transforms_label\": True,\n",
    "#         \"transformed_data_format\": \"text/csv\",\n",
    "#         \"sparse_encoding\": True\n",
    "#     },\n",
    "#     \"algorithm\": {\n",
    "#         \"name\": \"mlp\",\n",
    "#         \"training_resource_config\": {\n",
    "#             \"instance_type\": \"ml.m5.12xlarge\",\n",
    "#             \"instance_count\": 1,\n",
    "#         },\n",
    "#         \"candidate_specific_static_hyperparameters\": {\n",
    "#             \"num_categorical_features\": '2',\n",
    "#         }\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Candidates\n",
    "\n",
    "You have selected the following candidates (please run the cell below and click on the feature transformer links for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <table>\n",
       "            <tr><th>Candidate Name</th><th>Algorithm</th><th>Feature Transformer</th></tr>\n",
       "            <tr><th>dpp0-xgboost</th><td>xgboost</td><td><a href='automl-woobin-artifacts/generated_module/candidate_data_processors/dpp0.py'>dpp0.py</a></td></tr>\n",
       "<tr><th>dpp2-linear-learner</th><td>linear-learner</td><td><a href='automl-woobin-artifacts/generated_module/candidate_data_processors/dpp2.py'>dpp2.py</a></td></tr>\n",
       "            </table>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl_interactive_runner.display_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering pipeline consists of two SageMaker jobs:\n",
    "\n",
    "1. Generated trainable data transformer Python modules like [dpp0.py](automl-woobin-artifacts/generated_module/candidate_data_processors/dpp0.py), which has been downloaded to the local file system\n",
    "2. A **training** job to train the data transformers\n",
    "3. A **batch transform** job to apply the trained transformation to the dataset to generate the algorithm compatible data\n",
    "\n",
    "The transformers and its training pipeline are built using open sourced **[sagemaker-scikit-learn-container][]** and **[sagemaker-scikit-learn-extension][]**.\n",
    "\n",
    "[sagemaker-scikit-learn-container]: https://github.com/aws/sagemaker-scikit-learn-container\n",
    "[sagemaker-scikit-learn-extension]: https://github.com/aws/sagemaker-scikit-learn-extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Candidate Pipelines\n",
    "\n",
    "Each candidate pipeline consists of two steps, feature transformation and algorithm training.\n",
    "For efficiency first execute the feature transformation step which will generate a featurized dataset on S3\n",
    "for each pipeline.\n",
    "\n",
    "After each featurized dataset is prepared, execute a multi-algorithm tuning job that will run tuning jobs\n",
    "in parallel for each pipeline. This tuning job will execute training jobs to find the best set of\n",
    "hyper-parameters for each pipeline, as well as finding the overall best performing pipeline.\n",
    "\n",
    "### Run Data Transformation Steps\n",
    "\n",
    "Now you are ready to start execution all data transformation steps.  The cell below may take some time to finish,\n",
    "feel free to go grab a cup of coffee. To expedite the process you can set the number of `parallel_jobs` to be up to 10.\n",
    "Please check the account limits to increase the limits before increasing the number of jobs to run in parallel.\n",
    "\n",
    "각 후보 파이프라인은 기능 변환과 알고리듬 훈련이라는 두 단계로 구성된다. 효율성을 위해 먼저 각 파이프라인에 대해 S3에 피처화된 데이터 세트를 생성하는 피처 변환 단계를 실행한다.\n",
    "\n",
    "각 기능화된 데이터 세트가 준비되면 각 파이프라인에 대해 튜닝 작업을 병렬로 실행하는 다중 알고리즘 튜닝 작업을 실행합니다. 이 튜닝 작업은 각 파이프라인에 대한 최적의 하이퍼 파라미터 세트를 찾을 뿐만 아니라 전체적으로 가장 성능이 좋은 파이프라인을 찾기 위해 교육 작업을 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".2023-02-15 14:53:01,706 INFO root: [Worker_0:dpp0-xgboost]Executing step: train_data_transformer\n",
      "2023-02-15 14:53:01,859 INFO sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: latest.\n",
      "2023-02-15 14:53:01,875 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 14:53:01,877 INFO sagemaker: Creating training-job with name: automl-woo-notebook-run-15-14-44-38-dpp0-train-15-14-52-54\n",
      "\n",
      "2023-02-15 14:53:02 Starting - Starting the training job.......\n",
      "2023-02-15 14:53:17 Starting - Preparing the instances for training...............\n",
      "2023-02-15 14:54:01 Downloading - Downloading input data...........\n",
      "2023-02-15 14:54:32 Training - Downloading the training image.\n",
      "2023-02-15 14:54:37 Training - Training image download completed. Training in progress.........!\n",
      ".\n",
      "2023-02-15 14:55:08 Uploading - Uploading generated training model.\n",
      "2023-02-15 14:55:18 Completed - Training job completed\n",
      "2023-02-15 14:55:25,749 INFO root: [Worker_0:dpp0-xgboost]Executing step: create_transformer_model\n",
      "2023-02-15 14:55:25,795 INFO sagemaker: Creating model with name: sagemaker-sklearn-automl-2023-02-15-14-55-25-751\n",
      "2023-02-15 14:55:33,293 INFO root: [Worker_0:dpp0-xgboost]Executing step: perform_data_transform\n",
      "2023-02-15 14:55:33,296 INFO sagemaker: Creating transform job with name: automl-woo-notebook-run-15-14-44-38-dpp0-transform-15-14-52-54\n",
      "...........................................................!\n",
      "2023-02-15 15:00:31,503 INFO root: Successfully fit data transformer for dpp0-xgboost\n",
      "2023-02-15 15:00:39,509 INFO root: [Worker_0:dpp2-linear-learner]Executing step: train_data_transformer\n",
      "2023-02-15 15:00:39,668 INFO sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: latest.\n",
      "2023-02-15 15:00:39,686 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 15:00:39,689 INFO sagemaker: Creating training-job with name: automl-woo-notebook-run-15-14-44-38-dpp2-train-15-14-52-54\n",
      "\n",
      "2023-02-15 15:00:39 Starting - Starting the training job...\n",
      "2023-02-15 15:00:57 Starting - Preparing the instances for training........\n",
      "2023-02-15 15:01:45 Downloading - Downloading input data....\n",
      "2023-02-15 15:02:10 Training - Downloading the training image.\n",
      "2023-02-15 15:02:20 Training - Training image download completed. Training in progress........\n",
      "2023-02-15 15:03:01 Uploading - Uploading generated training model.\n",
      "2023-02-15 15:03:07 Completed - Training job completed\n",
      "2023-02-15 15:03:19,703 INFO root: [Worker_0:dpp2-linear-learner]Executing step: create_transformer_model\n",
      "2023-02-15 15:03:19,823 INFO sagemaker: Creating model with name: sagemaker-sklearn-automl-2023-02-15-15-03-19-704\n",
      "2023-02-15 15:03:28,313 INFO root: [Worker_0:dpp2-linear-learner]Executing step: perform_data_transform\n",
      "2023-02-15 15:03:28,315 INFO sagemaker: Creating transform job with name: automl-woo-notebook-run-15-14-44-38-dpp2-transform-15-14-52-54\n",
      "...........................................................!\n",
      "2023-02-15 15:08:26,211 INFO root: Successfully fit data transformer for dpp2-linear-learner\n",
      "2023-02-15 15:08:26,213 INFO root: Successfully fit 2 data transformers\n"
     ]
    }
   ],
   "source": [
    "automl_interactive_runner.fit_data_transformers(parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Algorithm Hyperparameter Tuning\n",
    "\n",
    "Now that the algorithm compatible transformed datasets are ready, you can start the multi-algorithm model tuning job\n",
    "to find the best predictive model. The following algorithm training job configuration for each\n",
    "algorithm is auto-generated by the AutoML Job as part of the recommendation.\n",
    "\n",
    "이제 알고리즘 호환 변환된 데이터 세트가 준비되었으므로 다중 알고리즘 모델 튜닝 작업을 시작하여 최적의 예측 모델을 찾을 수 있습니다. 각 알고리즘에 대한 다음 알고리즘 교육 작업 구성은 권장 사항의 일부로 AutoML Job에 의해 자동으로 생성됩니다.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Available Knobs</strong>\n",
    "\n",
    "1. Hyperparameter ranges\n",
    "2. Objective metrics\n",
    "3. Recommended static algorithm hyperparameters.\n",
    "\n",
    "Please refers to [Xgboost tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) and [Linear learner tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html) for detailed explanations of the parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML recommendation job has recommended the following hyperparameters, objectives and accuracy metrics for\n",
    "the algorithm and problem type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_OBJECTIVE_METRICS = {\n",
    "    'xgboost': 'validation:f1_binary',\n",
    "    'linear-learner': 'validation:binary_f_beta',\n",
    "    'mlp': 'validation:binary_f_beta',\n",
    "}\n",
    "\n",
    "STATIC_HYPERPARAMETERS = {\n",
    "    'xgboost': {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'accuracy,f1_binary,auc,balanced_accuracy,precision,recall,logloss',\n",
    "        'scale_pos_weight': 6.109333333333334,\n",
    "        '_kfold': 5,\n",
    "    },\n",
    "    'linear-learner': {\n",
    "        'predictor_type': 'binary_classifier',\n",
    "        'ml_application': 'linear_learner',\n",
    "        'loss_function': 'SoftmaxCrossEntropyLoss',\n",
    "        'reporting_metrics': 'binary_classification_accuracy,binary_f_beta,roc_auc_score,balanced_accuracy,precision,recall,binary_logloss',\n",
    "        'positive_example_weight_mult': 6.109333333333334,\n",
    "        'eval_metric': 'binary_f_beta',\n",
    "        'kfold': 5,\n",
    "        'num_cv_rounds': 3,\n",
    "        'prediction_storage_mode': 'store_cv_avg_predictions',\n",
    "    },\n",
    "    'mlp': {\n",
    "        'problem_type': 'binary_classification',\n",
    "        'positive_example_weight_mult': 6.109333333333334,\n",
    "        'ml_application': 'mlp',\n",
    "        'use_batchnorm': 'true',\n",
    "        'activation': 'relu',\n",
    "        'warmup_epochs': 10,\n",
    "        'reporting_metrics': 'accuracy,binary_f_1,roc_auc,balanced_accuracy,precision,recall,logloss',\n",
    "        'eval_metric': 'binary_f_1',\n",
    "        'kfold': 5,\n",
    "        'prediction_storage_mode': 'store_cv_avg_predictions',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tunable hyperparameters search ranges are recommended for the Multi-Algo tuning job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import CategoricalParameter, ContinuousParameter, IntegerParameter\n",
    "\n",
    "ALGORITHM_TUNABLE_HYPERPARAMETER_RANGES = {\n",
    "    'xgboost': {\n",
    "        'num_round': IntegerParameter(64, 1024, scaling_type='Logarithmic'),\n",
    "        'max_depth': IntegerParameter(2, 8, scaling_type='Logarithmic'),\n",
    "        'eta': ContinuousParameter(1e-3, 1.0, scaling_type='Logarithmic'),\n",
    "        'gamma': ContinuousParameter(1e-6, 64.0, scaling_type='Logarithmic'),\n",
    "        'min_child_weight': ContinuousParameter(1e-6, 32.0, scaling_type='Logarithmic'),\n",
    "        'subsample': ContinuousParameter(0.5, 1.0, scaling_type='Linear'),\n",
    "        'colsample_bytree': ContinuousParameter(0.3, 1.0, scaling_type='Linear'),\n",
    "        'lambda': ContinuousParameter(1e-6, 2.0, scaling_type='Logarithmic'),\n",
    "        'alpha': ContinuousParameter(1e-6, 2.0, scaling_type='Logarithmic'),\n",
    "    },\n",
    "    'linear-learner': {\n",
    "        'mini_batch_size': IntegerParameter(128, 512, scaling_type='Linear'),\n",
    "        'wd': ContinuousParameter(1e-12, 1e-2, scaling_type='Logarithmic'),\n",
    "        'learning_rate': ContinuousParameter(1e-6, 1e-2, scaling_type='Logarithmic'),\n",
    "    },\n",
    "    'mlp': {\n",
    "        'mini_batch_size': IntegerParameter(128, 512, scaling_type='Linear'),\n",
    "        'learning_rate': ContinuousParameter(1e-6, 1e-2, scaling_type='Logarithmic'),\n",
    "        'weight_decay': ContinuousParameter(1e-12, 1e-2, scaling_type='Logarithmic'),\n",
    "        'dropout_prob': ContinuousParameter(0.25, 0.5, scaling_type='Linear'),\n",
    "        'embedding_size_factor': ContinuousParameter(0.65, 0.95, scaling_type='Linear'),\n",
    "        'network_type': CategoricalParameter(['feedforward', 'widedeep']),\n",
    "        'layers': CategoricalParameter(['256', '50, 25', '100, 50', '200, 100', '256, 128', '300, 150', '200, 100, 50']),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Multi-Algorithm Tuner Input\n",
    "\n",
    "To use the multi-algorithm HPO tuner, prepare some inputs and parameters. Prepare a dictionary whose key is the name of the trained pipeline candidates and the values are respectively:\n",
    "\n",
    "1. Estimators for the recommended algorithm\n",
    "2. Hyperparameters search ranges\n",
    "3. Objective metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_algo_tuning_parameters = automl_interactive_runner.prepare_multi_algo_parameters(\n",
    "    objective_metrics=ALGORITHM_OBJECTIVE_METRICS,\n",
    "    static_hyperparameters=STATIC_HYPERPARAMETERS,\n",
    "    hyperparameters_search_ranges=ALGORITHM_TUNABLE_HYPERPARAMETER_RANGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you prepare the inputs data to the multi-algo tuner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_algo_tuning_inputs = automl_interactive_runner.prepare_multi_algo_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Multi-Algorithm Tuner\n",
    "\n",
    "With the recommended Hyperparameter ranges and the transformed dataset, create a multi-algorithm model tuning job\n",
    "that coordinates hyper parameter optimizations across the different possible algorithms and feature processing strategies.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Available Knobs</strong>\n",
    "\n",
    "1. Tuner strategy: [Bayesian](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Bayesian_optimization), [Random Search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search)\n",
    "2. Objective type: `Minimize`, `Maximize`, see [optimization](https://en.wikipedia.org/wiki/Mathematical_optimization)\n",
    "3. Max Job size: the max number of training jobs HPO would be launching to run experiments. Note the default value is **250**\n",
    "    which is the default of the managed flow.\n",
    "4. Parallelism. Number of jobs that will be executed in parallel. Higher value will expedite the tuning process.\n",
    "    Please check the account limits to increase the limits before increasing the number of jobs to run in parallel\n",
    "5. Please use a different tuning job name if you re-run this cell after applied customizations.\n",
    "</div>\n",
    "\n",
    "튜너 전략: 베이지안, 랜덤 검색\n",
    "목표 유형: 최소화, 최대화, 최적화 참조\n",
    "최대 작업 크기: HPO가 실험을 실행하기 위해 실행할 최대 교육 작업 수입니다. 기본값은 관리되는 흐름의 기본값인 250입니다.\n",
    "평행성. 병렬로 실행될 작업 수입니다. 값이 클수록 튜닝 프로세스가 빨라집니다. 병렬로 실행할 작업 수를 늘리기 전에 계정 제한을 확인하여 제한을 늘리십시오\n",
    "사용자 지정을 적용한 후 이 셀을 다시 실행하는 경우 다른 튜닝 작업 이름을 사용하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "base_tuning_job_name = \"{}-tuning\".format(AUTOML_LOCAL_RUN_CONFIG.local_automl_job_name)\n",
    "\n",
    "tuner = HyperparameterTuner.create(\n",
    "    base_tuning_job_name=base_tuning_job_name,\n",
    "    strategy='Bayesian',\n",
    "    objective_type='Maximize',\n",
    "    max_parallel_jobs=1, #4\n",
    "    max_jobs=8, #250\n",
    "    **multi_algo_tuning_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Multi-Algorithm Tuning\n",
    "\n",
    "Now you are ready to start running the **Multi-Algo Tuning** job. After the job is finished, store the tuning job name which you use to select models in the next section.\n",
    "The tuning process will take some time, please track the progress in the Amazon SageMaker Hyperparameter tuning jobs console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-15 15:12:00,397 INFO sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: latest.\n",
      "2023-02-15 15:12:00,426 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 15:12:00,429 INFO sagemaker.image_uris: Defaulting to the only supported framework/algorithm version: latest.\n",
      "2023-02-15 15:12:00,448 INFO sagemaker.image_uris: Ignoring unnecessary instance type: None.\n",
      "2023-02-15 15:12:00,449 WARNING sagemaker.estimator: No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "2023-02-15 15:12:00,455 INFO sagemaker: Creating hyperparameter tuning job with name: automl-woo-notebook--230215-1512\n",
      "...........................................................................................................................................................................................................................................................................!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Tuning Job automl-woo-notebook--230215-1512 started, please track the progress from [here](https://ap-northeast-2.console.aws.amazon.com/sagemaker/home?region=ap-northeast-2#/hyper-tuning-jobs/automl-woo-notebook--230215-1512)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Run tuning\n",
    "tuner.fit(inputs=multi_algo_tuning_inputs, include_cls_metadata=None)\n",
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "\n",
    "display(\n",
    "    Markdown(f\"Tuning Job {tuning_job_name} started, please track the progress from [here](https://{AUTOML_LOCAL_RUN_CONFIG.region}.console.aws.amazon.com/sagemaker/home?region={AUTOML_LOCAL_RUN_CONFIG.region}#/hyper-tuning-jobs/{tuning_job_name})\"))\n",
    "\n",
    "# Wait for tuning job to finish\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Deployment\n",
    "\n",
    "This section guides you through the model selection process. Afterward, you construct an inference pipeline\n",
    "on Amazon SageMaker to host the best candidate.\n",
    "\n",
    "Because you executed the feature transformation and algorithm training in two separate steps, you now need to manually\n",
    "link each trained model with the feature transformer that it is associated with. When running a regular Amazon\n",
    "SageMaker Autopilot job, this will automatically be done for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Job Result Overview\n",
    "\n",
    "The performance of each candidate pipeline can be viewed as a Pandas dataframe. For more interactive usage please\n",
    "refers to [model tuning monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mini_batch_size</th>\n",
       "      <th>wd</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingJobDefinitionName</th>\n",
       "      <th>alpha</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>lambda</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>num_round</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automl-woo-notebook--230215-1512-001-7db457db</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.793990</td>\n",
       "      <td>2023-02-15 15:13:16+00:00</td>\n",
       "      <td>2023-02-15 15:14:54+00:00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>dpp0-xgboost</td>\n",
       "      <td>0.588088</td>\n",
       "      <td>0.391735</td>\n",
       "      <td>0.144569</td>\n",
       "      <td>0.027247</td>\n",
       "      <td>1.195854</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.896994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automl-woo-notebook--230215-1512-005-657df1b4</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.779480</td>\n",
       "      <td>2023-02-15 15:24:04+00:00</td>\n",
       "      <td>2023-02-15 15:25:51+00:00</td>\n",
       "      <td>107.0</td>\n",
       "      <td>dpp0-xgboost</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.562214</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>1.402580</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.606925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automl-woo-notebook--230215-1512-003-1ee687a7</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.769810</td>\n",
       "      <td>2023-02-15 15:18:59+00:00</td>\n",
       "      <td>2023-02-15 15:20:31+00:00</td>\n",
       "      <td>92.0</td>\n",
       "      <td>dpp0-xgboost</td>\n",
       "      <td>1.315062</td>\n",
       "      <td>0.466512</td>\n",
       "      <td>0.710914</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.166086</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.139939</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.997384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automl-woo-notebook--230215-1512-006-378d72d3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.683780</td>\n",
       "      <td>2023-02-15 15:26:01+00:00</td>\n",
       "      <td>2023-02-15 15:29:10+00:00</td>\n",
       "      <td>189.0</td>\n",
       "      <td>dpp0-xgboost</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.865630</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>23.241531</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>748.0</td>\n",
       "      <td>0.889711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009778</td>\n",
       "      <td>391.0</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>automl-woo-notebook--230215-1512-008-dc7782f7</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>2023-02-15 15:33:36+00:00</td>\n",
       "      <td>2023-02-15 15:34:38+00:00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>dpp2-linear-learner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003217</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>automl-woo-notebook--230215-1512-007-f78f06f2</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.448669</td>\n",
       "      <td>2023-02-15 15:29:44+00:00</td>\n",
       "      <td>2023-02-15 15:32:37+00:00</td>\n",
       "      <td>173.0</td>\n",
       "      <td>dpp2-linear-learner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002460</td>\n",
       "      <td>503.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>automl-woo-notebook--230215-1512-002-f343e535</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.418440</td>\n",
       "      <td>2023-02-15 15:15:21+00:00</td>\n",
       "      <td>2023-02-15 15:18:34+00:00</td>\n",
       "      <td>193.0</td>\n",
       "      <td>dpp2-linear-learner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000299</td>\n",
       "      <td>478.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>automl-woo-notebook--230215-1512-004-6945c16e</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>2023-02-15 15:20:55+00:00</td>\n",
       "      <td>2023-02-15 15:23:37+00:00</td>\n",
       "      <td>162.0</td>\n",
       "      <td>dpp2-linear-learner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  mini_batch_size        wd  \\\n",
       "7            NaN              NaN       NaN   \n",
       "3            NaN              NaN       NaN   \n",
       "5            NaN              NaN       NaN   \n",
       "2            NaN              NaN       NaN   \n",
       "0       0.009778            391.0  0.004683   \n",
       "1       0.003217            271.0  0.000002   \n",
       "6       0.002460            503.0  0.000041   \n",
       "4       0.000299            478.0  0.000002   \n",
       "\n",
       "                                 TrainingJobName TrainingJobStatus  \\\n",
       "7  automl-woo-notebook--230215-1512-001-7db457db         Completed   \n",
       "3  automl-woo-notebook--230215-1512-005-657df1b4         Completed   \n",
       "5  automl-woo-notebook--230215-1512-003-1ee687a7         Completed   \n",
       "2  automl-woo-notebook--230215-1512-006-378d72d3         Completed   \n",
       "0  automl-woo-notebook--230215-1512-008-dc7782f7         Completed   \n",
       "1  automl-woo-notebook--230215-1512-007-f78f06f2         Completed   \n",
       "6  automl-woo-notebook--230215-1512-002-f343e535         Completed   \n",
       "4  automl-woo-notebook--230215-1512-004-6945c16e         Completed   \n",
       "\n",
       "   FinalObjectiveValue         TrainingStartTime           TrainingEndTime  \\\n",
       "7             0.793990 2023-02-15 15:13:16+00:00 2023-02-15 15:14:54+00:00   \n",
       "3             0.779480 2023-02-15 15:24:04+00:00 2023-02-15 15:25:51+00:00   \n",
       "5             0.769810 2023-02-15 15:18:59+00:00 2023-02-15 15:20:31+00:00   \n",
       "2             0.683780 2023-02-15 15:26:01+00:00 2023-02-15 15:29:10+00:00   \n",
       "0             0.458150 2023-02-15 15:33:36+00:00 2023-02-15 15:34:38+00:00   \n",
       "1             0.448669 2023-02-15 15:29:44+00:00 2023-02-15 15:32:37+00:00   \n",
       "6             0.418440 2023-02-15 15:15:21+00:00 2023-02-15 15:18:34+00:00   \n",
       "4             0.247788 2023-02-15 15:20:55+00:00 2023-02-15 15:23:37+00:00   \n",
       "\n",
       "   TrainingElapsedTimeSeconds TrainingJobDefinitionName     alpha  \\\n",
       "7                        98.0              dpp0-xgboost  0.588088   \n",
       "3                       107.0              dpp0-xgboost  0.000949   \n",
       "5                        92.0              dpp0-xgboost  1.315062   \n",
       "2                       189.0              dpp0-xgboost  0.000004   \n",
       "0                        62.0       dpp2-linear-learner       NaN   \n",
       "1                       173.0       dpp2-linear-learner       NaN   \n",
       "6                       193.0       dpp2-linear-learner       NaN   \n",
       "4                       162.0       dpp2-linear-learner       NaN   \n",
       "\n",
       "   colsample_bytree       eta      gamma    lambda  max_depth  \\\n",
       "7          0.391735  0.144569   0.027247  1.195854        4.0   \n",
       "3          0.562214  0.001375   0.000168  1.402580        7.0   \n",
       "5          0.466512  0.710914   0.000007  0.166086        3.0   \n",
       "2          0.865630  0.035534  23.241531  0.000001        2.0   \n",
       "0               NaN       NaN        NaN       NaN        NaN   \n",
       "1               NaN       NaN        NaN       NaN        NaN   \n",
       "6               NaN       NaN        NaN       NaN        NaN   \n",
       "4               NaN       NaN        NaN       NaN        NaN   \n",
       "\n",
       "   min_child_weight  num_round  subsample  \n",
       "7          0.011248      125.0   0.896994  \n",
       "3          0.002304      118.0   0.606925  \n",
       "5          0.139939       66.0   0.997384  \n",
       "2          0.000032      748.0   0.889711  \n",
       "0               NaN        NaN        NaN  \n",
       "1               NaN        NaN        NaN  \n",
       "6               NaN        NaN        NaN  \n",
       "4               NaN        NaN        NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "SAGEMAKER_SESSION = AUTOML_LOCAL_RUN_CONFIG.sagemaker_session\n",
    "SAGEMAKER_ROLE = AUTOML_LOCAL_RUN_CONFIG.role\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(\n",
    "    tuner.latest_tuning_job.name, sagemaker_session=SAGEMAKER_SESSION)\n",
    "\n",
    "df_tuning_job_analytics = tuner_analytics.dataframe()\n",
    "\n",
    "# Sort the tuning job analytics by the final metrics value\n",
    "df_tuning_job_analytics.sort_values(\n",
    "    by=['FinalObjectiveValue'],\n",
    "    inplace=True,\n",
    "    ascending=False if tuner.objective_type == \"Maximize\" else True)\n",
    "\n",
    "# Show detailed analytics for the top 20 models\n",
    "df_tuning_job_analytics.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best training job can be selected as below:\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong>Tips: </strong>\n",
    "You could select alternative job by using the value from `TrainingJobName` column above and assign to `best_training_job` below\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Multi Algorithm HPO training job name is automl-woo-notebook--230215-1512-001-7db457db\n"
     ]
    }
   ],
   "source": [
    "attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=SAGEMAKER_SESSION)\n",
    "best_training_job = attached_tuner.best_training_job()\n",
    "\n",
    "print(\"Best Multi Algorithm HPO training job name is {}\".format(best_training_job))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Best Training Job with Feature Pipelines\n",
    "\n",
    "Finally, deploy the best training job to Amazon SageMaker along with its companion feature engineering models.\n",
    "At the end of the section, you get an endpoint that's ready to serve online inference or start batch transform jobs!\n",
    "\n",
    "기능 파이프라인과 최상의 교육 작업 연결\n",
    "마지막으로 Amazon SageMaker의 동반 기능 엔지니어링 모델과 함께 최상의 교육 작업을 배포합니다. 섹션이 끝나면 온라인 추론을 제공하거나 일괄 변환 작업을 시작할 준비가 된 엔드포인트가 제공됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy a [PipelineModel](https://sagemaker.readthedocs.io/en/stable/pipeline.html) that has multiple containers of the following:\n",
    "\n",
    "1. Data Transformation Container: a container built from the model we selected and trained during the data transformer sections\n",
    "2. Algorithm Container: a container built from the trained model we selected above from the best HPO training job.\n",
    "3. Inverse Label Transformer Container: a container that converts numerical intermediate prediction value back to non-numerical label value.\n",
    "\n",
    "Get both best data transformation model and algorithm model from best training job and create an pipeline model:\n",
    "\n",
    "다음과 같은 여러 컨테이너가 있는 Pipeline Model을 배포합니다:\n",
    "\n",
    "데이터 변환 컨테이너: 데이터 변환 섹션에서 선택하고 교육한 모델을 기반으로 구축된 컨테이너\n",
    "Algorithm Container: 최상의 HPO 교육 작업에서 위에서 선택한 훈련된 모델을 기반으로 제작된 컨테이너입니다.\n",
    "Inverse Label Transformer Container: 숫자 중간 예측 값을 다시 숫자가 아닌 레이블 값으로 변환하는 컨테이너입니다.\n",
    "최고의 교육 작업을 통해 최고의 데이터 변환 모델과 알고리즘 모델을 모두 확보하고 파이프라인 모델을 생성하십시오:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import PipelineModel\n",
    "from sagemaker_automl import select_inference_output\n",
    "\n",
    "# Get a data transformation model from chosen candidate\n",
    "best_candidate = automl_interactive_runner.choose_candidate(df_tuning_job_analytics, best_training_job)\n",
    "best_data_transformer_model = best_candidate.get_data_transformer_model(role=SAGEMAKER_ROLE, sagemaker_session=SAGEMAKER_SESSION)\n",
    "\n",
    "# Our first data transformation container will always return recordio-protobuf format\n",
    "best_data_transformer_model.env[\"SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT\"] = 'application/x-recordio-protobuf'\n",
    "# Add environment variable for sparse encoding\n",
    "if best_candidate.data_transformer_step.sparse_encoding:\n",
    "    best_data_transformer_model.env[\"AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF\"] = '1'\n",
    "\n",
    "# Get a algo model from chosen training job of the candidate\n",
    "algo_estimator = Estimator.attach(best_training_job)\n",
    "best_algo_model = algo_estimator.create_model(**best_candidate.algo_step.get_inference_container_config())\n",
    "\n",
    "# Final pipeline model is composed of data transformation models and algo model and an\n",
    "# inverse label transform model if we need to transform the intermediates back to non-numerical value\n",
    "model_containers = [best_data_transformer_model, best_algo_model]\n",
    "if best_candidate.transforms_label:\n",
    "    model_containers.append(best_candidate.get_data_transformer_model(\n",
    "        transform_mode=\"inverse-label-transform\",\n",
    "        role=SAGEMAKER_ROLE,\n",
    "        sagemaker_session=SAGEMAKER_SESSION))\n",
    "\n",
    "# This model can emit response ['predicted_label', 'probability', 'labels', 'probabilities']. To enable the model to emit one or more\n",
    "# of the response content, pass the keys to `output_key` keyword argument in the select_inference_output method.\n",
    "\n",
    "model_containers = select_inference_output(\"BinaryClassification\", model_containers, output_keys=['predicted_label'])\n",
    "\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=\"AutoML-{}\".format(AUTOML_LOCAL_RUN_CONFIG.local_automl_job_name),\n",
    "    role=SAGEMAKER_ROLE,\n",
    "    models=model_containers,\n",
    "    vpc_config=AUTOML_LOCAL_RUN_CONFIG.vpc_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Best Pipeline\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Available Knobs</strong>\n",
    "\n",
    "1. You can customize the initial instance count and instance type used to deploy this model.\n",
    "2. Endpoint name can be changed to avoid conflict with existing endpoints.\n",
    "\n",
    "</div>\n",
    "\n",
    "Finally, deploy the model to SageMaker to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.deploy(initial_instance_count=1,\n",
    "                      instance_type='ml.m5.2xlarge',\n",
    "                      endpoint_name=pipeline_model.name,\n",
    "                      wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you could visit the sagemaker\n",
    "[endpoint console page](https://ap-northeast-2.console.aws.amazon.com/sagemaker/home?region=ap-northeast-2#/endpoints) to find the deployed endpoint (it'll take a few minutes to be in service).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <strong>To rerun this notebook, delete or change the name of your endpoint!</strong> <br>\n",
    "If you rerun this notebook, you'll run into an error on the last step because the endpoint already exists. You can either delete the endpoint from the endpoint console page or you can change the <code>endpoint_name</code> in the previous code block.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
